---
title: "Class vs Total Coverage"
output: html_notebook
---
```{r}
## Read Data from csv files

labels = read.csv('TCGA-PANCAN-HiSeq-801x20531/labels.csv')[,-1] # labels[i] = tumor type for patient #i (5 types: BRCA, COAD, KIRC, LUAD, PRAD)
data = read.csv('TCGA-PANCAN-HiSeq-801x20531/data.csv')[,-1]
data = data[,which(colSums(data!=0)>0)] # removing genes with zero expression level across all patients
colnames(data) = NULL
# 801 patients, 20264 genes
```

```{r}
# Set up data
set.seed(278)
X = as.matrix(data) 
Y = labels
n = length(Y)
tumors <- c("BRCA", "COAD", "KIRC", "LUAD", "PRAD")
NUM_GENES <- dim(X)[2]

num_train <- 400
num_test <- n-num_train

train <- sample(1:n, num_train)
test <- setdiff(1:n, train)

Xtest = X[test,]
Ytest = Y[test]
Xtrain = X[train,]
Ytrain = Y[train]

# Split data for split-conformal inference step
ntrain <- length(train)
I1_ind <- sample(1:ntrain, floor(ntrain/2))
I2_ind <- setdiff(1:ntrain, I1_ind)
I1 <- Xtrain[I1_ind,]
I1y <-  Ytrain[I1_ind]
I2 <- Xtrain[I2_ind,]
I2y <- Ytrain[I2_ind]


num_neighbors <- 10
alpha <- 0.1
```

We will use kNN to estimate the conditional probability distribution p(Y|X) with a choice of k = 10. In particular, the scope of our questions here are not so related to the probability distribution. We are mainly interested in the class vs total coverage given that the same estimation procedure is used in both. We have (or will) investigated the probability distribution elsewhere. 
```{r}
# We will use kNN to fit p(y|x)
library(FNN)

kNN_condl_prob <- function(y, x, num_neighbors, xTrain, yTrain){
  nbhrs = get.knnx(xTrain, x, k=num_neighbors) 
  return(length(which(yTrain[nbhrs$nn.index] == y))/num_neighbors)
}

# Compute thresholds for class coverage - Here the phat is the estimated probabilities
# of the I2 points all pertaining to one class.
# Hence, this function returns t_y, the class specific threshold value
thlds <- function(phat, alpha) {
  S = sort(phat, decreasing=FALSE)
  return(S[ceiling((length(phat)+1)*alpha-1)])
}
```

We first do a simple feature selection step to reduce the huge dimensionality of the dataset. We use a straightforward variance threshold and only take the genes above the 0.9 quantile in variance. Like with the probability estimation, our method of feature selection is not the main focus of our investigation here; we are interested in class vs total coverage tradeoffs given that the same feature selection and estimation methods are used in both. 
```{r}
# Feature selection
feat_vars <- apply(scale(I1, center=FALSE, scale=colSums(I1)), 2, var)
qtile = 0.90
high_var_ind <- which(feat_vars >= quantile(feat_vars, qtile, na.rm=TRUE))
print(length(high_var_ind))
```
This leaves us with $2019$ genes. 

In this investigation, we are primarily interested in the tradeoffs associated with guaranteeing class specific coverage over total coverage. More precisely, the authors (Sadinle et al.) give methods to bound the total misclassification rate as well as conditional on the class. We imagine there are tradeoffs in the ambiguity of a classifier when one decides to guarantee class coverage. In particular, we expect that the classifier becomes more conservative(i.e.  more ambigious) in order to ensure that atleast one of its predictions is accurate. In contrast, the total coverage only requires that the classifier predict with $1-\alpha$ accuracy **on average**. So if the probability distribution of the data points is heavily biased towards one class whereas another class is much more rare, the classifier is likely to make many more mistakes in the rare class since it will be balanced out by the predominance of the frequent class. We are interested in how this tradeoff plays in the current gene dataset. This is certainly of interest to practioners who may use this algorithm to actually diagnose patients. 
  The properties of the tradeoffs are certainly dependent on how the data is split between I1/I2. We will investigate this relationship as well.

We are interested in the following questions:
  * How does the per class error rate of total coverage compare to the guaranteed coverage rate per class? How does this depend on $\alpha$?
  * How do the ambiguous classifications compare among the class specific coverage and total coverage classifiers?
  * What is the effect of I1/I2 split (in terms of number of samples) on the ambiguity/null? 
  * What about pathological configurations of I2, where there are very few samples of one class? How robust is this method?
  * (Can we ask an inference question?)

We first investigate the misclassification rate.
Here, we use a conventional I1/I2 split of 50/50. Further, alpha = 0.1. Recall that kNN is used (k = 10) and feat selection is var threshold.
```{r}
# Estimate the distribution of p(y|x) - only I1 is allowed for fitting p(y|x)
phat_I2 <- rep(0, length(I2_ind))
for(i in 1:(length(I2_ind))) {
  phat_I2[i] <- kNN_condl_prob(I2y[i], t(I2[i, high_var_ind]), num_neighbors, I1[,high_var_ind], I1y)
}

# Get threshold for total classifier
total_t <- thlds(phat_I2, alpha)

# Get threshold for class specific classifier
t_y <- rep(0, 5)
for(i in 1:5){
  t_y[i] <- thlds(phat_I2[which(as.double(I2y) == i)], alpha)  
}
```


```{r}
# Total Classifier - Classify test data
results = matrix(0L, ncol=5, nrow=length(test))
for(i in 1:length(test)) {
  x = Xtest[i, high_var_ind]
  
  index = 1
  for(tum in 1:5){
    p <- kNN_condl_prob(tumors[tum], t(x), num_neighbors, I1[,high_var_ind], I1y)
    if(p >= total_t){
      results[i, index] = 1
    }
    index <- index+1
  }
}
# Count number of points with null classification
num_null <- length(which(rowSums(results) == 0))
print(num_null)

# Count number of points with more than 1 classification
num_ambig <- length(which(rowSums(results) > 1))
print(num_ambig)
```
```{r}
# Compute total coverage rate
coverage_rate <- 0
for(i in 1:length(test)) {
  if(results[i, Ytest[i]] == 1) {
    coverage_rate <- coverage_rate + 1
  }
}
coverage_rate <- coverage_rate/length(test)
coverage_rate
```
```{r}
# Compute class specific coverage rate
class_coverage_rates <- rep(0, 5)
for(tum in 1:5){
  tum_ind = which(Ytest == tumors[tum])
  
  for(i in tum_ind) {
    if(results[i, Ytest[i]] == 1) {
      class_coverage_rates[tum] <- class_coverage_rates[tum] + 1
    }
  }
  class_coverage_rates[tum] <- class_coverage_rates[tum]/length(tum_ind)
}
class_coverage_rates
```

Oh an interesting result! We see that tumor type 4 has a lower coverage rate than all the others (and certainly falls below $\alpha$).
For reference, the tumor types are ordered as 
```{r}
tumors
```
so we see that the classifier does pretty poorly on the "LUAD" tumor type when optimized for total coverage. To understand why, we looked at the distribution of tumor types.

```{r}
barplot(table(I1y)/length(I1y), ylim=c(0, 1), main="I1")
barplot(table(I2y)/length(I2y), ylim=c(0, 1), main="I2")
barplot(table(Ytest)/length(Ytest), ylim=c(0,1), main="Test")
```
Hmmm, we have the worst coverage on LUAD but it seems to be well represented in all of parts of the data post-split. Interestingly enough, COAD has the least representation in all three parts, yet the classifier classifies it with 93% accuracy. What is happening here? It is most likely due to the structure of the features and their relation to the LUAD. (I bet if you plotted the first two principal components, LUAD would be very hard to separate from another tumor class). 

We now confirm that our class specific classifier satisfies the $\alpha$ bound.
```{r}
# Class Specific Classifier - Classify test data
class_results = matrix(0L, ncol=5, nrow=length(test))
for(i in 1:length(test)) {
  x = Xtest[i, high_var_ind]
  
  index = 1
  for(tum in 1:5){
    p <- kNN_condl_prob(tumors[tum], t(x), num_neighbors, I1[,high_var_ind], I1y)
    if(p >= t_y[tum]){
      class_results[i, index] = 1
    }
    index <- index+1
  }
}
# Count number of points with null classification
num_null <- length(which(rowSums(class_results) == 0))
print(num_null)

# Count number of points with more than 1 classification
num_ambig <- length(which(rowSums(class_results) > 1))
print(num_ambig)
```
```{r}
# Compute class specific coverage rate
class_spec_coverage_rates <- rep(0, 5)
for(tum in 1:5){
  tum_ind = which(Ytest == tumors[tum])
  
  for(i in tum_ind) {
    if(class_results[i, Ytest[i]] == 1) {
      class_spec_coverage_rates[tum] <- class_spec_coverage_rates[tum] + 1
    }
  }
  class_spec_coverage_rates[tum] <- class_spec_coverage_rates[tum]/length(tum_ind)
}
class_spec_coverage_rates
```
As expected, the class specific classifier satisfies the misclassification bound (and 88.4% is admissible whereas 84.2% (total coverage) as the deviation between 88.4% and 90% is plausibly explained by finite test set). Interestingly, we do not pay in terms of the ambiguity tradeoff. We will investigate to what extent we can gain confidence in our prediction conditonal on class while maintaining low ambiguity. In particular, this will change with the $\alpha$ and $I1/I2$ configurations.

We now examine this effect with respect to $\alpha$ constraint. First, we will see the effects on the training data and choose hypotheses to test. 
```{r}
# Test training data to see effects and choose hypotheses to test
alphas <- seq(0.05, 0.25, 0.0125)
train_total_ambigs <- rep(0, length(alphas))
train_total_nulls <- rep(0, length(alphas))
train_class_ambigs <- rep(0, length(alphas))
train_class_nulls <- rep(0, length(alphas))
train_total_coverage <- matrix(0, nrow=length(alphas), ncol=5)
train_class_condl_coverage <- matrix(0, nrow=length(alphas), ncol=5)

for(a in 1:length(alphas)) {
  print(alphas[a])
  
  # Get thresholds
  total_t <- thlds(phat_I2, alphas[a])
  t_y <- rep(0, 5)
  for(i in 1:5){
    t_y[i] <- thlds(phat_I2[which(as.double(I2y) == i)], alphas[a])  
  }
  
  ###### Total Classifier - Classify train data ###############
  results = matrix(0L, ncol=5, nrow=length(train))
  for(i in 1:length(train)) {
    x = Xtrain[i, high_var_ind]
    
    for(tum in 1:5){
      p <- kNN_condl_prob(tumors[tum], t(x), num_neighbors, I1[,high_var_ind], I1y)
      if(p >= total_t){
        results[i, tum] = 1
      }
    }
  }
  # Count number of points with null classification
  num_null <- length(which(rowSums(results) == 0))
  
  # Count number of points with more than 1 classification
  num_ambig <- length(which(rowSums(results) > 1))
  
  # Compute class specific coverage rate
  class_coverage_rates <- rep(0, 5)
  for(tum in 1:5){
    tum_ind = which(Ytrain == tumors[tum])
    
    for(i in tum_ind) {
      if(results[i, Ytrain[i]] == 1) {
        class_coverage_rates[tum] <- class_coverage_rates[tum] + 1
      }
    }
    class_coverage_rates[tum] <- class_coverage_rates[tum]/length(tum_ind)
  }
  
  # Store results
  train_total_ambigs[a] <- num_ambig
  train_total_nulls[a] <- num_null
  train_total_coverage[a,] <- class_coverage_rates
  ############################################################
  
  ########### Class Condl Classifier - Classify train data #########
  # Class Specific Classifier - Classify train data
  class_results = matrix(0L, ncol=5, nrow=length(train))
  for(i in 1:length(train)) {
    x = Xtrain[i, high_var_ind]
    
    for(tum in 1:5){
      p <- kNN_condl_prob(tumors[tum], t(x), num_neighbors, I1[,high_var_ind], I1y)
      if(p >= t_y[tum]){
        class_results[i, tum] = 1
      }
    }
  }
  # Count number of points with null classification
  num_null <- length(which(rowSums(class_results) == 0))
  
  # Count number of points with more than 1 classification
  num_ambig <- length(which(rowSums(class_results) > 1))
  
  # Compute class specific coverage rate
  class_spec_coverage_rates <- rep(0, 5)
  for(tum in 1:5){
    tum_ind = which(Ytrain == tumors[tum])
    
    for(i in tum_ind) {
      if(class_results[i, Ytrain[i]] == 1) {
        class_spec_coverage_rates[tum] <- class_spec_coverage_rates[tum] + 1
      }
    }
    class_spec_coverage_rates[tum] <- class_spec_coverage_rates[tum]/length(tum_ind)
  }

  # Store results
  train_class_ambigs[a] <- num_ambig
  train_class_nulls[a] <- num_null
  train_class_condl_coverage[a,] <- class_spec_coverage_rates
  ################################################################
}
```

```{r}
matplot(alphas, train_total_coverage, pch=1, ylim=c(0.6, 1), ylab="Coverage Rate", xlab="alpha", main="Class Specific Coverage - Total Classifier - Train")
lines(alphas, 1-alphas, col=10)
legend("bottomleft", legend=c(tumors, "Error Bound"), pch=c(rep(1, 5), 19), col=c(1:5, 10))

matplot(alphas, train_class_condl_coverage, pch=1, ylim=c(0.6, 1), ylab="Coverage Rate", xlab="alpha", main="Class Specific Coverage - Class Conditional Classifier - Train")
lines(alphas, 1-alphas, col=10)
legend("bottomleft", legend=c(tumors, "Error Bound"), pch=c(rep(1, 5), 19), col=c(1:5, 10))
```

```{r}
plot(alphas, train_total_ambigs, ylim=c(0, 400), ylab="# Ambiguous", xlab="alpha", main="Total Classifier - Ambiguity by alpha - Train")
plot(alphas, train_class_ambigs, ylim=c(0, 400), ylab="# Ambiguous", xlab="alpha", main="Class Specific - Ambiguity by alpha - Train")
plot(alphas, train_total_nulls, ylim=c(0, 400), ylab="# Null", xlab="alpha", main="Total Classifier - Null by alpha - Train")
plot(alphas, train_class_nulls, ylim=c(0, 400), ylab="# Null", xlab="alpha", main="Class Specific - Null by alpha - Train")
```
The results of the training set show that LUAD is not classified very well by the total classifier. Further, we see that the class specific classifier has tremendous ambiguity for very extreme values of $\alpha$ (see below for intuition - especially about affording error). From these results, we hypothesize that LUAD is difficult to classify. Since $\alpha$ is large, the total classifier can afford to incur the error rate, and so does not output ambiguous classifications. To this end, we will test some hypotheses on the test data. In particular, we hypothesize that the LUAD data points are highly correlated with some other tumor type. Further, we hypothesize that the mean distance in feature space between LUAD points and another tumor type is significantly small. Note here that we do not have a good explanation (or formulation of question) for why LUAD is the one to be misclassified, rather than the tumor type it is similar to. This will require more thought... 

Now we run the same on the test set. 
```{r}
total_ambigs <- rep(0, length(alphas))
total_nulls <- rep(0, length(alphas))
class_ambigs <- rep(0, length(alphas))
class_nulls <- rep(0, length(alphas))
total_coverage <- matrix(0, nrow=length(alphas), ncol=5)
class_condl_coverage <- matrix(0, nrow=length(alphas), ncol=5)

for(a in 1:length(alphas)) {
  print(alphas[a])
  
  # Get thresholds
  total_t <- thlds(phat_I2, alphas[a])
  t_y <- rep(0, 5)
  for(i in 1:5){
    t_y[i] <- thlds(phat_I2[which(as.double(I2y) == i)], alphas[a])  
  }
  
  ###### Total Classifier - Classify test data ###############
  results = matrix(0L, ncol=5, nrow=length(test))
  for(i in 1:length(test)) {
    x = Xtest[i, high_var_ind]
    
    for(tum in 1:5){
      p <- kNN_condl_prob(tumors[tum], t(x), num_neighbors, I1[,high_var_ind], I1y)
      if(p >= total_t){
        results[i, tum] = 1
      }
    }
  }
  # Count number of points with null classification
  num_null <- length(which(rowSums(results) == 0))
  
  # Count number of points with more than 1 classification
  num_ambig <- length(which(rowSums(results) > 1))
  
  # Compute class specific coverage rate
  class_coverage_rates <- rep(0, 5)
  for(tum in 1:5){
    tum_ind = which(Ytest == tumors[tum])
    
    for(i in tum_ind) {
      if(results[i, Ytest[i]] == 1) {
        class_coverage_rates[tum] <- class_coverage_rates[tum] + 1
      }
    }
    class_coverage_rates[tum] <- class_coverage_rates[tum]/length(tum_ind)
  }
  
  # Store results
  total_ambigs[a] <- num_ambig
  total_nulls[a] <- num_null
  total_coverage[a,] <- class_coverage_rates
  print(total_coverage)
  ############################################################
  
  ########### Class Condl Classifier - Classify test data #########
  # Class Specific Classifier - Classify test data
  class_results = matrix(0L, ncol=5, nrow=length(test))
  for(i in 1:length(test)) {
    x = Xtest[i, high_var_ind]
    
    for(tum in 1:5){
      p <- kNN_condl_prob(tumors[tum], t(x), num_neighbors, I1[,high_var_ind], I1y)
      if(p >= t_y[tum]){
        class_results[i, tum] = 1
      }
    }
  }
  # Count number of points with null classification
  num_null <- length(which(rowSums(class_results) == 0))
  
  # Count number of points with more than 1 classification
  num_ambig <- length(which(rowSums(class_results) > 1))
  
  # Compute class specific coverage rate
  class_spec_coverage_rates <- rep(0, 5)
  for(tum in 1:5){
    tum_ind = which(Ytest == tumors[tum])
    
    for(i in tum_ind) {
      if(class_results[i, Ytest[i]] == 1) {
        class_spec_coverage_rates[tum] <- class_spec_coverage_rates[tum] + 1
      }
    }
    class_spec_coverage_rates[tum] <- class_spec_coverage_rates[tum]/length(tum_ind)
  }

  # Store results
  class_ambigs[a] <- num_ambig
  class_nulls[a] <- num_null
  class_condl_coverage[a,] <- class_spec_coverage_rates
  print(class_condl_coverage)
  ################################################################
}

```


```{r}
matplot(alphas, total_coverage, pch=1, ylim=c(0.6, 1), ylab="Coverage Rate", xlab="alpha", main="Class Specific Coverage - Total Classifier")
lines(alphas, 1-alphas, col=10)
legend("bottomleft", legend=c(tumors, "Error Bound"), pch=c(rep(1, 5), 19), col=c(1:5, 10))
```

```{r}
matplot(alphas, class_condl_coverage, pch=1, ylim=c(0.6, 1), ylab="Coverage Rate", xlab="alpha", main="Class Specific Coverage - Class Conditional Classifier")
lines(alphas, 1-alphas, col=10)
legend("bottomleft", legend=c(tumors, "Error Bound"), pch=c(rep(1, 5), 19), col=c(1:5, 10))
```

Looks good! We guarantee our error bound with the class specific classifier, whereas the total classifier does pretty terribly for the LUAD tumor. Interestingly, all tumors except for LUAD are classified within the error bound for the total classifier, and we recapitulate the result found in the training set. 
  Why might this be the case? For some reason, it is easier to correctly predict all of the other tumor types but fail for LUAD. Further, observe that the total classifier has no ambiguous classificatoins for larger values of $\alpha$. This means that LUAD is being incorrectly classified as another tumor, and the total classifier does not output an ambiguous instance since $\alpha$ is large and so can afford to make predictions without tempering with ambiguity. 
  Another interesting observation is that the LUAD classification fails appreciably only for large $\alpha > 0.1$, and drops from there. This seems to be counterintuitive, in that the total classifier fails to meet a more generous constraint. In contrast, for very strict $\alpha$, the total classifier is able to meet the error bound. However, this observation can be explained as follows. With a larger $\alpha$, the total classifier is able to classify with less ambiguity and can afford to incur the misclassification loss. Now since the total classifer only guarantees a bound on the **total** misclassification (i.e. on average), the classifier ends up misclassifying many instances of LUAD while making up for this inaccuracy by accurately predicting the other tumor types. Further, since $\alpha$ is large, the classifier can afford to incur larger misclassfication error, and thus dumps all of it into LUAD. As such, the total classifier meets the total coverage constraint, but falls very short for LUAD. 
  This is controlled for in the Class Specific Classifier. We see that in all tumor types, the $\alpha$ bound is satisfied. The slight deviations in the coverage rate for COAD are slight enough to be explained by the finiteness of the test data. 
    The guarantee in coverage rate comes at a tremendous cost for low alpha. In particular, observe that we have $401$ ambiguous classifications; this is the entire test set. However, observe that for $alpha > 0.1125$, there is an immediate decrease in ambiguity to obtain $0$ ambiguous classifications. Therefore, the tradeoff between ambiguity only exists at very extreme values of $alpha$, and drops off instantly when there is room to afford errors in classification. In particular, we see that for large $\alpha$, the class specific classifier guarantees the desired bound while matching the total classifier in ambiguity. This seems to be a stricly dominant procedure (albeit this is after looking at results of test data), but the results match our intuition. With large $\alpha$, there is enough room for error that the class specific classifier can minimize ambiguity while still satisfying the desired error bound. 

```{r}
plot(alphas, total_ambigs, ylim=c(0, 400), ylab="# Ambiguous", xlab="alpha", main="Total Classifier - Ambiguity by alpha")
plot(alphas, class_ambigs, ylim=c(0, 400), ylab="# Ambiguous", xlab="alpha", main="Class Specific - Ambiguity by alpha")
```


```{r}
plot(alphas, total_nulls, ylim=c(0, 400), ylab="# Null", xlab="alpha", main="Total Classifier - Null by alpha")
plot(alphas, class_nulls, ylim=c(0, 400), ylab="# Null", xlab="alpha", main="Class Specific - Null by alpha")
```

As we said above, we are now interested in why classifying LUAD is difficult for the total classifier. In particular, it seems that the difficulty of classifying LUAD is what drives the ambiguity of class specific classifier for extreme values of $\alpha$.
  As we decided by looking at the training data, we will test some hypotheses for the results observed on the test data. In particular, we hypothesize that the mean distance in feature space between LUAD points and another tumor type is significantly smaller than the mean distance to other tumor types. Recall that we are estimating $p(Y|X)$ with $kNN$ and so we are interested in relative distance rather than absolute distance. We also ask if this classification of LUAD is consistently below the guarantee bound for the total classifier (at some fixed alpha). Also, we ask if the ambiguity count of the class specific classifier at alpha=0.1 is consistently 0 (or close to). 
  
(Note here that we do not have a good explanation (or formulation of question) for why LUAD is the one to be misclassified, rather than the tumor type it is similar to. This will require more thought.)

More concretely, the hypotheses we test:
  * Mean distance in feature space between LUAD and another tumor type is significantly smaller than mean distance to other tumor types.
  * Misclassification of LUAD is significantly below error bound in total classifier. 
  * Ambiguity of class specific classifier is 0 at alpha=0.1


We proceed in order. The first question has the following hypothesis testing structure:    
$$
  H_{0, i} : \text{Mean pairwise distance between LUAD and tumor type i is equal to mean distance between LUAD and non-tumor type i} \\
  H_{1,i} : \text{Mean pairwise distance between LUAD and tumor type } i \text{ individual's feature vectors is significantly smaller than mean distance to non-tumor type i}
$$

Here, $i \in \{1, 2, 3, 5}$ and correspond the ordered list of tumors ("BRCA", "COAD","KIRC", "LUAD", "PRAD"). Note that we do not consider $i = 4$ since distance between LUAD and LUAD is 0. Observe here that we have $4$ hypotheses to test. Further, note that we generated these hypotheses from looking at our training set, and are testing them on fresh test data. As such, no additional correction is needed.

Our test statistic is as follows. We compute Euclidean distance for each pair consisting of one gene vector from LUAD and one from tumor type i. We then compute the mean of these pairwise distances, and use this in the signficance testing. P-values are computed via permutation test.
  The permutation test is constructed as follows:
    1) First, compute mean distance between LUAD and tumor type i 
    2) Repeat 1000x:
        i) Permute all non LUAD individuals
       ii) Compute mean distance between LUAD and indices that were previously labeled tumor type i
      iii) Collect values to create distribution.
    3) Compute p-value
```{r}
library(proxy)

## Test distance hypothesis
nperm = 1000
pvals_perm = c()
luad_ind = which(Ytest == "LUAD")
non_luad_ind = which(Ytest != "LUAD")
for(tum in tumors) {
  if(tum == "LUAD"){
    next
  }
  tumor_ind <- which(Ytest == tum)
  non_tumor_ind <- which(Ytest != tum)
  
  mean_d <- mean((dist(Xtest[luad_ind, high_var_ind], Xtest[tumor_ind,high_var_ind])))
  print(mean_d)
  
  perm_d <- rep(0, nperm)
  perms <- apply(matrix(rnorm(nperm*(length(non_luad_ind))), nperm, length(non_luad_ind)), 1, order) # each column is one permutation
  for(j in 1:nperm){
    perm_d[j] <- mean((dist(Xtest[luad_ind, high_var_ind], Xtest[non_luad_ind[perms[,j]],high_var_ind])))
  }
  pvals_perm = c(pvals_perm, (1 + sum(perm_d <= mean_d))/(1+nperm)) # distance is always nonnegative so don't have to worry about negatives
  #pvals_perm = c(pvals_perm, (1 + rowSums(abs(meandiffs_perm) >= abs(meandiffs)%*%t(rep(1, nperm))))/(1+nperm))
}

```
```{r}
pvals_perm ## Still needs to be corrected
```

We see that LUAD is very close to BRCA and PRAD (which our dumb visualization shows below). However, we cannot conclude anything yet as these pvalues must be corrected due to the multiple testing setting. We will do this correction after we have computed pvalues for everything we wish to test. 

```{r}
# A really dumb visualization. Take the first 400 high_var genes as features and do PCA. Project onto first 2 components
# We see that L and B are pretty mashed together. However, when you add in all the other genes (and pcs) they separate. 
tpca <- princomp(Xtest[,high_var_ind[1:400]])
Ytest.color <- rep('red',n)
Ytest.color[which(Y==2)] <- 'gold'
Ytest.color[which(Y==3)] <- 'blue'


plot(tpca$score[,1], tpca$score[,2],col=Ytest.color,pch=as.character(Ytest),xlab='Principal Component 1',ylab='Principal Component 2',
	cex.axis=1.5,cex.lab=1.8,lwd=3,cex=1.5,main="Iris Data",cex.main=2)
```

We now test the significance of misclassification and ambiguity. we could also test to see if the misclassification is significantly bad of the test classifier -> repeatedly draw 200 size samples from test data, run test classifier, and see how far away from the guarantee bound it really is => we can fix an alpha and do for all tumors.

```{r}
## Permute 1:400
alphas = seq(0.05, 0.15, 0.0125)
perm_ind <- order(rnorm(length(test)))
mean_total_coverage_rates = matrix(nrow=length(alphas), ncol=5)
sdev_total_coverage_rates = matrix(nrow=length(alphas), ncol=5)
for(a in 1:length(alphas)){
  # Get thresholds
  total_t <- thlds(phat_I2, alphas[a])
  t_y <- rep(0, 5)
  for(i in 1:5){
    t_y[i] <- thlds(phat_I2[which(as.double(I2y) == i)], alphas[a])  
  }
  
  total_ambigs <- rep(0, 20)
  total_nulls <- rep(0, 20)
  total_coverage <- matrix(0, nrow=20, ncol=5)
  
  for(batch in 1:20){ 
    test_batch = Xtest[perm_ind[(20*(batch-1)+1):(20*batch)],high_var_ind]
    test_batch_class = Ytest[perm_ind[(20*(batch-1)+1):(20*batch)]]
    
    ###### Total Classifier - Classify batch of test data ###############
    results = matrix(0L, ncol=5, nrow=20)
    for(i in 1:20) {
      x = test_batch[i,]
      
      for(tum in 1:5){
        p <- kNN_condl_prob(tumors[tum], t(x), num_neighbors, I1[,high_var_ind], I1y)
        if(p >= total_t){
          results[i, tum] = 1
        }
      }
    }
    
    # Count number of points with null classification
    prop_null <- length(which(rowSums(results) == 0))/20
    
    # Count number of points with more than 1 classification
    prop_ambig <- length(which(rowSums(results) > 1))/20
    
    # Compute class specific coverage rate
    class_coverage_rates <- rep(0, 5)
    for(tum in 1:5){
      tum_ind = which(test_batch_class == tumors[tum])
      if(length(tum_ind) == 0){
        class_coverage_rates[tum] <- 1
      }
      else{
        for(j in 1:length(tum_ind)) {
          if(results[tum_ind[j], tum] == 1) {
            class_coverage_rates[tum] <- class_coverage_rates[tum] + 1
          }
        }
        class_coverage_rates[tum] <- class_coverage_rates[tum]/length(tum_ind)
      }
    }
    
    # Store results
    total_ambigs[batch] <- prop_ambig
    total_nulls[batch] <- prop_null
    total_coverage[batch,] <- class_coverage_rates
  }
  
  mean_total_coverage_rates[a,] <- colMeans(total_coverage)
  sdev_total_coverage_rates[a, ] <- apply(total_coverage, 2, sd)
}

```

```{r}
# Test hypotheses using t-test with 19 degrees of freedom
t_pvals <- matrix(0, nrow=length(alphas), ncol=5)
for(a in 1:length(alphas)){
  for(tum in 1:5) {
    # n = 10 and df = 19
    stat <- (mean_total_coverage_rates[a, tum] - (1-alphas[a]))/(sdev_total_coverage_rates[a, tum]/(20**0.5))
    t_pvals[a, tum] = pt(stat, df=19)
  }
}
print(t_pvals)
```


We now test the hypothesis that the average proportion of ambiguous classifications is greater than 0
```{r}
mean_class_ambig = rep(0, length(alphas))
sdev_class_ambig = rep(0, length(alphas))
for(a in 1:length(alphas)){
  # Get thresholds
  t_y <- rep(0, 5)
  for(i in 1:5){
    t_y[i] <- thlds(phat_I2[which(as.double(I2y) == i)], alphas[a])  
  }
  
  ambigs <- rep(0, 20)
  nulls <- rep(0, 20)

  for(batch in 1:20){ 
    test_batch = Xtest[perm_ind[(20*(batch-1)+1):(20*batch)],high_var_ind]
    test_batch_class = Ytest[perm_ind[(20*(batch-1)+1):(20*batch)]]
    
    ###### Total Classifier - Classify batch of test data ###############
    class_results = matrix(0L, ncol=5, nrow=length(test))
    for(i in 1:20) {
      x = test_batch[i,]
      
      for(tum in 1:5){
        p <- kNN_condl_prob(tumors[tum], t(x), num_neighbors, I1[,high_var_ind], I1y)
        if(p >= t_y[tum]){
          class_results[i, tum] = 1
        }
      }
    }
    # Count number of points with null classification
    prop_null <- length(which(rowSums(class_results) == 0))/20
    
    # Count number of points with more than 1 classification
     prop_ambig <- length(which(rowSums(class_results) > 1))/20
    
    # Store results
    ambigs[batch] <- prop_ambig
    nulls[batch] <- prop_null
  }
  
  mean_class_ambig[a] <- mean(ambigs)
  sdev_class_ambig[a] <- sd(ambigs)
}
```

```{r}
print(alphas)
print(mean_class_ambig)
print(sdev_class_ambig)
```
We see that there is 0 standard deviation in the number of ambiguous classifications for each value of alpha. This corresponds to a p-value of 1. We shall include this in our BH correction in order to accurately deal with the multiple testing problem. 


```{r}
# pvals_perm
# t_pvals
# rep(1, length(alphas))
adjusted <- p.adjust(c(pvals_perm, c(t(t_pvals)), rep(1, length(alphas))), method=c("BH"))
adjusted_pvals_perm <- adjusted[1:length(pvals_perm)]
adjusted_t_pvals <- matrix(adjusted[(length(pvals_perm)+1):(length(pvals_perm)+length(c(t_pvals)))], nrow=dim(t_pvals)[1], ncol=dim(t_pvals)[2],byrow=TRUE)
```

```{r}
print(adjusted_pvals_perm)
print(adjusted_t_pvals)
```


